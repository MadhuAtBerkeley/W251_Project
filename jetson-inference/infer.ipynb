{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face detection and recognition inference pipeline\n",
    "\n",
    "The following example illustrates how to use the `facenet_pytorch` python package to perform face detection and recogition on an image dataset using an Inception Resnet V1 pretrained on the VGGFace2 dataset.\n",
    "\n",
    "The following Pytorch methods are included:\n",
    "* Datasets\n",
    "* Dataloaders\n",
    "* GPU/CPU processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.mtcnn import MTCNN, fixed_image_standardization\n",
    "#from models.inception_resnet_v1 import  InceptionResnetV1\n",
    "from models.utils import training\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "workers = 0 if os.name == 'nt' else 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine if an nvidia GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define MTCNN module\n",
    "\n",
    "Default params shown for illustration, but not needed. Note that, since MTCNN is a collection of neural nets and other code, the device must be passed in the following way to enable copying of objects when needed internally.\n",
    "\n",
    "See `help(MTCNN)` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Inception Resnet V1 module\n",
    "\n",
    "Set classify=True for pretrained classifier. For this example, we will use the model to output embeddings/CNN features. Note that for inference, it is important to set the model to `eval` mode.\n",
    "\n",
    "See `help(InceptionResnetV1)` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "model_path = './data/model_with_mask.pt'\n",
    "resnet = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a dataset and data loader\n",
    "\n",
    "We add the `idx_to_class` attribute to the dataset to enable easy recoding of label indices to identity names later one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "\n",
    "dataset = datasets.ImageFolder('./data/test_no_mask')\n",
    "dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()}\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn, num_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perfom MTCNN facial detection\n",
    "\n",
    "Iterate through the DataLoader object and detect faces and associated detection probabilities for each. The `MTCNN` forward method returns images cropped to the detected face, if a face was detected. By default only a single detected face is returned - to have `MTCNN` return all detected faces, set `keep_all=True` when creating the MTCNN object above.\n",
    "\n",
    "To obtain bounding boxes rather than cropped face images, you can instead call the lower-level `mtcnn.detect()` function. See `help(mtcnn.detect)` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /media/nvidia/NVME/pytorch/pytorch-v1.9.0/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "aligned = []\n",
    "names = []\n",
    "for i, (x, y) in enumerate(loader):\n",
    "    x_aligned, prob = mtcnn(x, return_prob=True)\n",
    "    if x_aligned is not None:\n",
    "        #print('Face detected with probability: {:8f}'.format(prob))\n",
    "        aligned.append(x_aligned)\n",
    "        names.append(dataset.idx_to_class[y])\n",
    "        if i==63:\n",
    "            break\n",
    "aligned = torch.stack(aligned).to(device)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder('./data/test_anchors')\n",
    "dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()}\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn, num_workers=workers)\n",
    "anchor_aligned = []\n",
    "anchors = []\n",
    "for x, y in loader:\n",
    "    x_aligned, prob = mtcnn(x, return_prob=True)\n",
    "    if x_aligned is not None:\n",
    "        #print('Face detected with probability: {:8f}'.format(prob))\n",
    "        anchor_aligned.append(x_aligned)\n",
    "        anchors.append(dataset.idx_to_class[y])\n",
    "anchor_aligned = torch.stack(anchor_aligned).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate image embeddings\n",
    "\n",
    "MTCNN will return images of faces all the same size, enabling easy batch processing with the Resnet recognition module. Here, since we only have a few images, we build a single batch and perform inference on it. \n",
    "\n",
    "For real datasets, code should be modified to control batch sizes being passed to the Resnet, particularly if being processed on a GPU. For repeated testing, it is best to separate face detection (using MTCNN) from embedding or classification (using InceptionResnetV1), as calculation of cropped faces or bounding boxes can then be performed a single time and detected faces saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del mtcnn\n",
    "model_path = './data/model_with_mask.pt'\n",
    "resnet = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = resnet(aligned).detach().cpu()\n",
    "anchor_emb = resnet(anchor_aligned).detach().cpu()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print distance matrix for classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           n000001    n000009    n000029    n000040    n000078    n000082  \\\n",
      "n000001  24.010246  53.063042  58.832172  63.359829  71.862518  64.357109   \n",
      "n000001  11.455333  48.455715  63.236080  64.244514  71.790527  63.548492   \n",
      "n000001  13.006725  50.070484  61.098518  63.326969  70.779251  62.579308   \n",
      "n000001  13.848736  55.900082  65.936813  67.159195  76.549568  67.890610   \n",
      "n000001  16.399628  49.769009  67.959435  70.051048  69.023140  63.268959   \n",
      "...            ...        ...        ...        ...        ...        ...   \n",
      "n000001  18.057278  53.439285  67.853035  66.568947  69.214592  62.930725   \n",
      "n000001  18.404099  51.451546  62.137474  63.991745  70.036270  64.997459   \n",
      "n000001  20.582541  51.010044  57.569706  60.114887  63.813461  58.306004   \n",
      "n000001  21.881485  57.605656  64.904343  64.309189  68.198601  64.356544   \n",
      "n000001  17.307190  53.731899  69.430634  70.710274  71.534424  67.446396   \n",
      "\n",
      "           n000106    n000129    n000129    n000148  ...    n000785  \\\n",
      "n000001  54.718792  51.237068  53.286499  56.585964  ...  55.396584   \n",
      "n000001  59.739258  47.357529  50.836845  53.544769  ...  48.811951   \n",
      "n000001  57.195683  48.026348  48.430225  55.525837  ...  47.313866   \n",
      "n000001  64.967407  53.262035  55.613754  60.471298  ...  55.043453   \n",
      "n000001  57.773281  47.949974  50.318340  55.320267  ...  50.697819   \n",
      "...            ...        ...        ...        ...  ...        ...   \n",
      "n000001  60.140087  47.053543  49.323250  57.618885  ...  54.811481   \n",
      "n000001  58.769222  50.033291  49.291016  56.992313  ...  49.167919   \n",
      "n000001  55.083569  47.334904  44.560722  53.762379  ...  44.581215   \n",
      "n000001  58.709126  49.915703  50.547020  59.224583  ...  51.285324   \n",
      "n000001  62.636681  47.412148  52.226933  57.937637  ...  52.783604   \n",
      "\n",
      "           n000836    n000838    n000854    n000912    n000928    n000945  \\\n",
      "n000001  55.642193  62.268311  52.543644  72.682587  60.131470  57.199665   \n",
      "n000001  61.439732  56.447010  52.358803  76.962395  67.388184  66.004951   \n",
      "n000001  58.122791  57.805462  50.740917  74.190186  61.925152  62.871635   \n",
      "n000001  63.169052  61.489357  56.073090  80.624649  68.288544  66.956436   \n",
      "n000001  63.977367  55.510006  52.980671  77.789810  66.419891  69.994133   \n",
      "...            ...        ...        ...        ...        ...        ...   \n",
      "n000001  65.943336  62.294220  58.175678  79.497993  69.817200  69.608833   \n",
      "n000001  59.700794  57.747654  50.760551  78.172157  61.934612  64.169250   \n",
      "n000001  54.217308  63.134377  54.571659  73.988945  59.553722  62.534866   \n",
      "n000001  54.039963  69.369316  62.193996  77.181297  64.760490  65.081879   \n",
      "n000001  68.257027  60.720707  60.318264  83.619583  72.831573  72.834007   \n",
      "\n",
      "           n000950    n000958    n000998  \n",
      "n000001  53.594860  46.934452  61.037945  \n",
      "n000001  57.284874  49.999107  60.541008  \n",
      "n000001  51.905384  48.198711  59.120373  \n",
      "n000001  58.382385  54.625534  66.181549  \n",
      "n000001  52.348732  50.652538  62.836121  \n",
      "...            ...        ...        ...  \n",
      "n000001  55.244312  55.153553  62.596947  \n",
      "n000001  49.350780  48.856831  61.898430  \n",
      "n000001  51.657768  43.531815  55.524487  \n",
      "n000001  58.190052  43.239521  61.645332  \n",
      "n000001  60.238312  53.813847  65.827858  \n",
      "\n",
      "[64 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "dists = [[(e1 - e2).norm().item() for e2 in anchor_emb] for e1 in embeddings]\n",
    "print(pd.DataFrame(dists, columns=anchors, index=names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
